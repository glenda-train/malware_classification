# ----------------------------------------------------------------------------------------------------------------------
# File containing the class with the definitions and functions to run the Gradient Boosting model for classification.
# ----------------------------------------------------------------------------------------------------------------------

import os, sys
import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.ensemble import GradientBoostingClassifier

from metrics import Metrics
from constants import SAVE_PATH
from pre_processing import preprocess_data
# ----------------------------------------------------------------------------------------------------------------------

# Gradient Boosting hyperparameters ranges that will be optimized
params = {
    "loss": ["deviance", "exponential"],
    "learning_rate": [0.001, 0.1],
    "n_estimators": [100, 200],
    "subsample": [1.0],
    "criterion": ["friedman_mse", "mse"],
    "min_samples_split": [2, 10],
    "min_samples_leaf": [1],
    "min_weight_fraction_leaf": [0.0],
    "max_depth": [100, 200],
    "min_impurity_decrease": [0.0],
    "init": [None],
    "random_state": [42],
    "max_features": [None],
    "verbose": [0],
    "max_leaf_nodes": [10, None],
    "warm_start": [False, True],
    "validation_fraction": [0.1],
    "n_iter_no_change": [None],
    "tol": [0.0001, 0.01],
    "ccp_alpha": [0.0, 0.1]
}
# ----------------------------------------------------------------------------------------------------------------------

# Class to build, train, predict and optimize the Gradient Boosting model
class GradientBoosting:

    # Defines default hyperparameters
    def __init__(self):
        self.loss = "deviance",
        self.learning_rate = 0.1,
        self.n_estimators = 100,
        self.subsample = 1.0,
        self.criterion = "friedman_mse",
        self.min_samples_split = 2,
        self.min_samples_leaf = 1,
        self.min_weight_fraction_leaf = 0.0,
        self.max_depth = 100,
        self.min_impurity_decrease = 0.0,
        self.init = None,
        self.random_state = 42,
        self.max_features = None,
        self.verbose = 0,
        self.max_leaf_nodes = None,
        self.warm_start = False,
        self.validation_fraction = 0.1,
        self.n_iter_no_change = None,
        self.tol = 0.0001,
        self.ccp_alpha = 0.0
        self.model = GradientBoostingClassifier()
        self.model_fit = None
    # ------------------------------------------------------------------------------------------------------------------

    # Build the model
    def build(self):
        self.model = GradientBoostingClassifier(
            loss=self.loss,
            learning_rate=self.learning_rate,
            n_estimators=self.n_estimators,
            subsample=self.subsample,
            criterion=self.criterion,
            min_samples_split=self.min_samples_split,
            min_samples_leaf=self.min_samples_leaf,
            min_weight_fraction_leaf=self.min_weight_fraction_leaf,
            max_depth=self.max_depth,
            min_impurity_decrease=self.min_impurity_decrease,
            init=self.init,
            random_state=self.random_state,
            max_features=self.max_features,
            verbose=self.verbose,
            max_leaf_nodes=self.max_leaf_nodes,
            warm_start=self.warm_start,
            validation_fraction=self.validation_fraction,
            n_iter_no_change=self.n_iter_no_change,
            tol=self.tol,
            ccp_alpha=self.ccp_alpha
        )

    # Function to set new hyperparameters after the model has been built
    def set_params(self,  loss="deviance", learning_rate=0.1, n_estimators=100, subsample=1.0, criterion="friedman_mse",
                          min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3,
                          min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0,
                          max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None,
                          tol=0.0001, ccp_alpha=0.0):

        self.loss = loss,
        self.learning_rate = learning_rate,
        self.n_estimators = n_estimators,
        self.subsample = subsample,
        self.criterion = criterion,
        self.min_samples_split = min_samples_split,
        self.min_samples_leaf = min_samples_leaf,
        self.min_weight_fraction_leaf = min_weight_fraction_leaf,
        self.max_depth = max_depth,
        self.min_impurity_decrease = min_impurity_decrease,
        self.init = init,
        self.random_state = random_state,
        self.max_features = max_features,
        self.verbose = verbose,
        self.max_leaf_nodes = max_leaf_nodes,
        self.warm_start = warm_start,
        self.validation_fraction = validation_fraction,
        self.n_iter_no_change = n_iter_no_change,
        self.tol = tol,
        self.ccp_alpha = ccp_alpha
        self.build()
    # ------------------------------------------------------------------------------------------------------------------

    # Train it
    def train(self, features, labels):
        self.model_fit = self.model.fit(features, labels)
    # ------------------------------------------------------------------------------------------------------------------

    # Predict it
    def predict(self, features):
        prediction = self.model.predict(features)
        return(prediction)
    # ------------------------------------------------------------------------------------------------------------------

    # Optimize the model with the hyperparameters intervals defined previously
    def optimize(self, parameters, features, labels):

        # Init the 5-fold object
        skf = StratifiedKFold(n_splits=5)

        # Init the classifier
        clf = GradientBoostingClassifier()

        # Init the Grid Search
        grid = GridSearchCV(estimator=clf, param_grid=parameters, verbose=3, cv=skf, return_train_score=True,
                            refit="recall_score")

        # Train it
        sys.stdout = open("gradient_boosting_tune.csv", "w")
        grid.fit(features, labels)

        # Get the results and filter it
        results = pd.DataFrame(grid.cv_results_)
        filtered_result = results[['params', 'rank_test_score', 'mean_test_score']]

        print("\nOverall Results:")
        print(filtered_result)

        print("\nBest Params:")
        print(grid.best_params_)

        print("\nBest Score:")
        print(grid.best_score_)

        sys.stdout.close()
    # ------------------------------------------------------------------------------------------------------------------

    # Get the predicted probability of testing data
    def predict_probabilities(self, test_features):
        score = self.model.predict_proba(test_features)[:, 1]
        return(score)
    # ------------------------------------------------------------------------------------------------------------------

# ----------------------------------------------------------------------------------------------------------------------

if __name__ == "__main__":

    # Read the dataset
    path = os.path.join(SAVE_PATH, "dataset.csv")
    dataset = pd.read_csv(path)

    # Preprocess the data
    train_set, val_set, test_set = preprocess_data(dataset)

    # Build the model
    grad_boosting = GradientBoosting()

    # Optimize it
    ext_train_set = {"features": np.vstack([train_set["features"], val_set["features"]]),
                     "labels": np.hstack([train_set["labels"], val_set["labels"]])}
    # grad_boosting.optimize(params, ext_train_set["features"], ext_train_set["labels"])

    # Train it
    grad_boosting.train(train_set["features"], train_set["labels"])

    # Init the 5-fold object
    cv = StratifiedKFold(n_splits=5)
    for i, (train, test) in enumerate(cv.split(test_set["features"], test_set["labels"])):

        # Train it
        grad_boosting.train(test_set["features"][train], test_set["labels"][train])

        # Get the probabilities of each class
        class_scores = grad_boosting.model.predict_proba(test_set["features"][test])

        # Make the prediction
        predicted = grad_boosting.predict(test_set["features"][test])
        expected = test_set["labels"][test]

        # Init the metrics class with the classes (to get the order)
        metrics = Metrics(grad_boosting.model.classes_)

        # Calculates the Accuracy, Precision, Recall and F1-Score for each class
        metrics.get_all_metrics(expected, predicted)

        # Calculates the confusion matrix
        metrics.get_confusion_matrix(expected, predicted, save=True, model="{}_gradient_boosting".format(i))

        # Calculates the AUC
        metrics.get_auc(expected, class_scores)

        # Calculates and plots the ROC
        metrics.get_roc(expected, class_scores)

        # Show Accuracy, Precision, Recall, F1-Score, AUC and Confusion Matrix
        metrics.print_all_metrics()
# ----------------------------------------------------------------------------------------------------------------------


