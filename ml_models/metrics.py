# ----------------------------------------------------------------------------------------------------------------------
# File containing the class for calculating metrics, such as Accuracy, Precision, Recall, F1-Score, AUC and ROC.
# ----------------------------------------------------------------------------------------------------------------------

import os
import numpy as np
import pandas as pd
import seaborn as sn
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, confusion_matrix

from constants import SAVE_PATH
from pre_processing import preprocess_data
# ----------------------------------------------------------------------------------------------------------------------

# Binarize the labels according to the probabilities order
def binarize_labels(labels, classes):

    binarized_labels = []
    for label in labels:

        # Adware class
        if (label == classes[0]):
            new_label = np.array([1, 0, 0, 0, 0])

        # Benign class
        elif (label == classes[1]):
            new_label = np.array([0, 1, 0, 0, 0])

        # Ransomware class
        elif (label == classes[2]):
            new_label = np.array([0, 0, 1, 0, 0])

        # Scareware class
        elif (label == classes[3]):
            new_label = np.array([0, 0, 0, 1, 0])

        # SMSMalware class
        else:
            new_label = np.array([0, 0, 0, 0, 1])

        binarized_labels.append(new_label)

    # Store everything inside a numpy array
    binarized_labels = np.array(binarized_labels)

    return(binarized_labels)
# ----------------------------------------------------------------------------------------------------------------------

# Class to gather the metrics
class Metrics:

    def __init__(self, classes):
        self.accuracy = 0
        self.precision = {}
        self.recall = {}
        self.f1_score = {}
        self.auc = {}
        self.confusion_matrix = None
        self.classes = classes

        # Create a dictionary to store the class specific metrics
        for class_ in classes:
            self.precision[class_] = 0
            self.recall[class_] = 0
            self.f1_score[class_] = 0
            self.auc[class_] = 0
    # ------------------------------------------------------------------------------------------------------------------

    # Get Accuracy, Precision, Recall and F1-Score, considering the multiclass problem
    def get_all_metrics(self, expected, predicted):

        # Get the accuracy (overall)
        self.accuracy = accuracy_score(expected, predicted)

        # Get the class specific metrics
        for class_ in self.classes:
            precision = precision_score(expected, predicted, average=None, labels=[class_])
            recall = recall_score(expected, predicted, average=None, labels=[class_])
            f1_score_ = f1_score(expected, predicted, average=None, labels=[class_])

            # Add it into a dictionary
            self.precision[class_] = precision[0]
            self.recall[class_] = recall[0]
            self.f1_score[class_] = f1_score_[0]
    # ------------------------------------------------------------------------------------------------------------------

    # Get the confusion matrix and show and save the plot
    def get_confusion_matrix(self, expected, predicted, save=False, model=""):

        # Get the confusion matrix
        cm = confusion_matrix(expected, predicted)
        self.confusion_matrix = cm

        # Show and save the plot
        if (save == True):

            # Format the axis labels
            columns = [class_[0] + class_[1:].lower() for class_ in self.classes]

            # Add all the information inside a DataFrame
            df_cm = pd.DataFrame(cm, index=columns, columns=columns)

            # Set the configs and generate the plot
            plt.figure(figsize=(10, 6))
            sn.heatmap(df_cm, annot=True, fmt='g')

            # Define the axis labels
            plt.xlabel("Classe Predita")
            plt.ylabel("Classe Esperada")

            plt.savefig("{}_confusion_matrix.png".format(model))
            plt.show()
    # ------------------------------------------------------------------------------------------------------------------

    # Calculates the Area Under the Curve (AUC) metric
    def get_auc(self, expected, probabilities):

        # Binarize the labels according to the probabilities order
        expected_labels = binarize_labels(expected, self.classes)

        # Generate AUC for each class
        for index, class_ in enumerate(self.classes):
            fpr, tpr, thresholds = roc_curve(expected_labels[:, index], probabilities[:, index])
            auc_ = auc(fpr, tpr)
            self.auc[class_] = auc_
    # ------------------------------------------------------------------------------------------------------------------

    # Calculates and plots the Receiver Operating Characteristic Curve (ROC)
    def get_roc(self, expected, probabilities, chosen_classes=None):

        # Binarize the labels according to the probabilities order
        expected_labels = binarize_labels(expected, self.classes)

        # Get the number of classes
        n_classes = len(self.classes)

        # Compute ROC curve and ROC area for each class
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(expected_labels[:, i], probabilities[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])

        # Compute micro-average ROC curve and ROC area
        fpr["micro"], tpr["micro"], _ = roc_curve(expected_labels.ravel(), probabilities.ravel())
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

        # Set the configs, axis labels and legends
        plt.figure()

        # If no class was chosen, then plot the ROC of all classes and the average ROC
        if(chosen_classes == None):
            chosen_classes = self.classes
            plt.plot(fpr["micro"], tpr["micro"], label='micro-average ROC curve (area = {0:0.2f})'
                                                       ''.format(roc_auc["micro"]))

        # Only plot the informed curves (by class name)
        for i in range(n_classes):
            if(self.classes[i] in chosen_classes):
                plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'
                                               ''.format(self.classes[i][0] + self.classes[i][1:].lower(), roc_auc[i]))

        # Plot the ROC curves
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Multiclass Receiver Operating Characteristic (ROC)')
        plt.legend(loc="lower right")
        plt.show()
    # ------------------------------------------------------------------------------------------------------------------

    # Print Accuracy, Precision, Recall, F1-Score, AUC and Confusion Matrix
    def print_all_metrics(self):

        print("\nAccuracy: {:.4}".format(self.accuracy))

        print("\nPrecision:")
        for class_ in self.classes:
            print("   - {}: {:.4}".format(class_[0] + class_[1:].lower(), self.precision[class_]))

        print("\nRecall:")
        for class_ in self.classes:
            print("   - {}: {:.4}".format(class_[0] + class_[1:].lower(), self.recall[class_]))

        print("\nF1-Score:")
        for class_ in self.classes:
            print("   - {}: {:.4}".format(class_[0] + class_[1:].lower(), self.f1_score[class_]))

        print("\nAUC:")
        for class_ in self.classes:
            print("   - {}: {:.4}".format(class_[0] + class_[1:].lower(), self.auc[class_]))

        print("\nConfusion Matrix:")
        print(self.confusion_matrix)
    # ------------------------------------------------------------------------------------------------------------------

# ----------------------------------------------------------------------------------------------------------------------