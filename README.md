# Ciência de Dados para Segurança - Classificação de Malwares

* Esse repositório contém arquivos para realizar o processo de Ciência de Dados no conjunto de dados disponibilizado pela Universidade de New Brunswick (UNB), chamado de [CIC-AndMal2017](https://www.unb.ca/cic/datasets/andmal2017.html).

* Os dados contêm dois tipos de arquivos (PCAPs e APKs), que estão associados à 4 classes diferentes de malwares:
  * Adware
  * Scareware
  * Ransomware
  * SMS Malware
  
* Além dessas classes, o conjunto de dados contêm amostras benignas dos anos de 2015, 2016 e 2017, totalizando 5 classes.

## Extração de Dados (PCAPs e APKs)
* Esse projeto contém códigos para realizar a extração de características dos dois tipos de dados brutos (PCAPs e APKs) e organizá-las em uma tabela no formato CSV com 481 características de 20.000 amostras rotuladas.
* Para os dados do tipo PCAP, 82 características foram extraídas utilizando a ferramenta [CICFlowMeter](https://github.com/ahlashkari/CICFlowMeter) e, em seguida, foram rotuladas utilizando as informações disponibilizadas pela UNB e organizadas em uma planilha CSV.
* Já os arquivos de APKs foram tratados com a ferramenta [APKTool](https://ibotpeaches.github.io/Apktool/) para obter o arquivo _AndroidManifest.xml_ de cada APK utilizando o processo de engenharia reversa.
* Com esse arquivo foi possível extrair as permissões requeridas por cada APK e montar uma lista booleana, marcando com 1 as permissões existentes em uma determinada APK, e 0 no caso de ausência dessa permissão.
* Essas listas foram adicionados à tabela criada inicialmente, associando elas aos PCAPs correspondentes.

## Balanceamento de Classes
* O arquivo CSV resultante da extração de dados apresentou um desbalanceamento de classes, com a classe SMS Malware contendo 212.084 amostras enquanto a benigna contava com 902.583 amostras.
* Para solucionar este problema, 4.000 amostras, escolhidas aleatoriamente, foram selecionadas de cada classe, totalizando um conjunto de dados final de 20.000 amostras. Essas amostras podem ser conferidas [aqui](class_balanced_csvs_apks).
* A tabela final acabou ficando muito extensa, por isso não foi adicionada ao repositório.

## Divisão do Dados
* Os dados foram divididos em conjuntos de treinamento e teste da seguinte forma:
  * **Conjunto de treinamento:** 80% dos dados
  * **Conjunto de teste:** 20% dos dados

## Pré-Processamento Modelos Baseados em Aprendizado de Máquina
* A primeira etapa de pré-processamento foi transformar os atributos em valores numéricos.
* Para isso, as características de data foram transformadas em Timestamp e os IPs em inteiros.
* Em seguida, foi realizada a normalização dos dados com a escala MinMax, mantendo todas as características dentre 0 e 1.
* Por fim, foi aplicada a Seleção de Características, com _Extra Trees_, em um subconjunto do conjunto de treinamento com 4.000 amostras.
* Foram selecionadas 50 características, contendo 2 advindas dos PCAPs (data e IP de origem) e 48 permissões como:
  * android.permission.ACCESS_COARSE_LOCATION
  * android.permission.SYSTEM_ALERT_WINDOW
  * android.permission.READ_PHONE_STATE
  * android.permission.SEND_SMS
  * android.permission.READ_CONTACTS
  * android.permission.READ_EXTERNAL_STORAGE
  * entre outras

## Modelos Baseados em Aprendizado de Máquina
* Modelos implementados:
  * Florestas Aleatórias (_Random Forests_)
  * KNN (K Nearest Neighbors)
  * Gradient Boosting

## Modelos Baseados em Aprendizado Profundo
* Modelos implementados:
  * ...
  * ...

## Métricas
* Métricas implementadas:
  * Acurácia
  * Precisão
  * Recall
  * F1-Score
  * AUC
  * Curvas ROC
  * Matrizes de Confusão

## Organização do Repositório:
* Dentro do diretório [data_extraction](data_extraction) são encontrados os arquivos para a extração de dados:
  * [pcap_features.py](data_extraction/pcap_features.py): TODO...
  * [apk_features.py](data_extraction/apk_features.py): TODO...
  * [unique_permissions.txt](data_extraction/unique_permissions.txt): TODO...

* Dentro do diretório [ml_models](ml_models) são encontrados os arquivos de pré-processamento, métricas, visualização e implementação dos modelos:
  * [class_distribution_plot](ml_models/class_distribution_plot.png): TODO...
  * [grid_search_results](ml_models/grid_search_results): TODO...
  * [confusion_matrices](ml_models/confusion_matrices): TODO...
  * [roc_curves](ml_models/roc_curves): TODO...
  * [constants.py](ml_models/constants.py): TODO...
  * [pre_processing.py](ml_models/pre_processing.py): TODO...
  * [visualization.py](ml_models/visualization.py): TODO...
  * [model_random_forest.py](ml_models/model_random_forest.py): TODO...
  * [model_knn.py](ml_models/model_knn.py): TODO...
  * [model_gradient_boosting.py](ml_models/model_gradient_boosting.py): TODO...
  * [metrics.py](ml_models/metrics.py): TODO...

## Como Instalar as Dependências:
* Para os diretórios [data_extraction](data_extraction) e [ml_models](ml_models), as dependências podem ser instaladas com (de preferência em um ambiente virtual):
  * ```pip install -r requirements.txt```

## Como executar:
* Para executar os modelos baseados em Aprendizado de Máquina, basta digitar:
  * **Florestas Aleatórias:** ```python model_random_forest.py```
  * **KNN:** ```python model_knn.py```
  * **Gradient Boosting:** ```python model_gradient_boosting.py```
* Ao fim da execução serão apresentadas as métricas dos modelos utilizando os hiperparâmetros já otimizados. 


