# Ciência de Dados para Segurança - Classificação de Malwares

* Esse repositório contém arquivos para realizar o processo de Ciência de Dados no conjunto de dados disponibilizado pela Universidade de _New Brunswick_ (UNB), chamado de [CIC-AndMal2017](https://www.unb.ca/cic/datasets/andmal2017.html).

* Os dados contêm dois tipos de arquivos (PCAPs e APKs), que estão associados à 4 classes diferentes de malwares:
  * Adware
  * Scareware
  * Ransomware
  * SMS Malware
  
* Além dessas classes, o conjunto de dados contêm amostras benignas dos anos de 2015, 2016 e 2017, totalizando 5 classes.

## Extração de Dados (PCAPs e APKs)
* Esse projeto contém códigos para realizar a extração de características dos dois tipos de dados brutos (PCAPs e APKs) e organizá-las em uma tabela no formato CSV com 481 características de 20.000 amostras rotuladas.
* Para os dados do tipo PCAP, 82 características foram extraídas utilizando a ferramenta [CICFlowMeter](https://github.com/ahlashkari/CICFlowMeter) e, em seguida, foram rotuladas utilizando as informações disponibilizadas pela UNB e organizadas em uma planilha CSV.
* Já os arquivos de APKs foram tratados com a ferramenta [APKTool](https://ibotpeaches.github.io/Apktool/) para obter o arquivo _AndroidManifest.xml_ de cada APK utilizando o processo de engenharia reversa.
* Com esse arquivo foi possível extrair as permissões requeridas por cada APK e montar uma lista booleana, marcando com 1 as permissões existentes em uma determinada APK, e 0 no caso de ausência dessa permissão.
* Essas listas foram adicionados à tabela criada inicialmente, associando elas aos PCAPs correspondentes.

## Balanceamento de Classes
* O arquivo CSV resultante da extração de dados apresentou um desbalanceamento de classes, com a classe SMS Malware contendo 212.084 amostras enquanto a benigna contava com 902.583 amostras.
* Para solucionar este problema, 4.000 amostras, escolhidas aleatoriamente, foram selecionadas de cada classe, totalizando um conjunto de dados final de 20.000 amostras. Essas amostras podem ser conferidas [aqui](class_balanced_csvs_apks).
* A tabela final acabou ficando muito extensa, por isso não foi adicionada ao repositório.

## Divisão do Dados
* Os dados foram divididos em conjuntos de treinamento e teste da seguinte forma:
  * **Conjunto de treinamento:** 80% dos dados
  * **Conjunto de teste:** 20% dos dados

## Pré-Processamento Modelos Baseados em Aprendizado de Máquina
* A primeira etapa de pré-processamento foi transformar os atributos em valores numéricos.
* Para isso, as características de data foram transformadas em Timestamp e os IPs em inteiros.
* Em seguida, foi realizada a normalização dos dados com a escala MinMax, mantendo todas as características dentre 0 e 1.
* Por fim, foi aplicada a Seleção de Características, com _Extra Trees_, em um subconjunto do conjunto de treinamento com 4.000 amostras.
* Foram selecionadas 50 características, contendo 2 advindas dos PCAPs (data e IP de origem) e 48 permissões como:
  * android.permission.ACCESS_COARSE_LOCATION
  * android.permission.SYSTEM_ALERT_WINDOW
  * android.permission.READ_PHONE_STATE
  * android.permission.SEND_SMS
  * android.permission.READ_CONTACTS
  * android.permission.READ_EXTERNAL_STORAGE
  * entre outras

## Modelos Baseados em Aprendizado de Máquina
* Modelos implementados:
  * Florestas Aleatórias (_Random Forests_)
  * KNN (_K Nearest Neighbors_)
  * _Gradient Boosting_

## Modelos Baseados em Aprendizado Profundo
* Modelos implementados:
  * Modelo Linear
  * Modelo Convolucional

## Métricas
* Métricas implementadas:
  * Acurácia
  * Precisão
  * Recall
  * F1-Score
  * AUC
  * Curvas ROC
  * Matrizes de Confusão

## Organização do Repositório:
* Arquivos gerais:
  * [class_balanced_csvs_cicflow](class_balanced_csvs_cicflow): diretório contendo o conjunto de dados com somente as características extraídas pelo CICFlowMeter. Contém as amostras de cada classe e a combinação balanceada com 20.000 amostras em [dataset.csv](class_balanced_csvs_cicflow/dataset.csv).
  * [class_balanced_csvs_apks](class_balanced_csvs_apks): diretório contendo o conjunto de dados com as características extraídas pelo CICFlowMeter e APKTool. Contém as amostras de cada classe e a combinação balanceada com 20.000 amostras em [dataset.csv](class_balanced_csvs_apks/dataset.csv).
  * [data_extraction](data_extraction):  diretório contendo os arquivos para realizar a extração de características e balanceamento dos dados.
  * [ml_models](ml_models): diretório contendo os arquivos para pré-processar, explorar, modelar e gerar resultados e métricas para os modelos baseados em Aprendizado de Máquina.
  * [requirements.txt](requirements.txt):
* Dentro do diretório [data_extraction](data_extraction) são encontrados os arquivos para a extração de dados:
  * [pcap_features.py](data_extraction/pcap_features.py): arquivo que contém as funções utilizadas para balancear e combinar os arquivos CSV extraídos extraídos com a ferramenta CICFlowMeter para cada um dos PCAPs disponibilizados pela UNB.
  * [apk_features.py](data_extraction/apk_features.py): arquivo que contém as funções utilizadas para extrair as permissões dos arquivos de APKs, disponibilizados pela UNB, além de funções para combinar essas permissões com os dados dos PCAPs.
  * [unique_permissions.txt](data_extraction/unique_permissions.txt): arquivo com as permissões únicas encontradas em todos as APKs.

* Dentro do diretório [ml_models](ml_models) são encontrados os arquivos de pré-processamento, métricas, visualização e implementação dos modelos:
  * [class_distribution_plot](ml_models/class_distribution_plot.png): arquivo com o gráfico da distribuição de classes do conjunto de dados, conjunto de treinamento e conjunto de teste.
  * [grid_search_results](ml_models/grid_search_results): diretório com os arquivos TXT contendo os resultados da Grid Search.
  * [confusion_matrices](ml_models/confusion_matrices): diretório contendo as matrizes de confusão encontradas pelos modelos em forma de gráfico.
  * [roc_curves](ml_models/roc_curves): diretório contendo as curvas ROC encontradas pelos modelos.
  * [constants.py](ml_models/constants.py): arquivo contendo as constantes utilizadas em todo o código.
  * [pre_processing.py](ml_models/pre_processing.py): arquivo contendo as funções para realizar o pré-processamento e divisão dos dados.
  * [visualization.py](ml_models/visualization.py): arquivo contendo funções de visualização, como a distribuição de classes.
  * [model_random_forest.py](ml_models/model_random_forest.py): arquivo contendo a classe e fluxo principal para executar o modelo de Florestas Aleatórias.
  * [model_knn.py](ml_models/model_knn.py): arquivo contendo a classe e fluxo principal para executar o modelo KNN.
  * [model_gradient_boosting.py](ml_models/model_gradient_boosting.py): arquivo contendo a classe e fluxo principal para executar o modelo _Gradient Boosting_.
  * [metrics.py](ml_models/metrics.py): arquivo contendo as funções para cálculo das métricas.

* Dentro do diretório [DeepLearning Models](DeepLearningModels):
  * [MyLinearModel](DeepLearningModels/MyLinearModel): Resultados do treinamento e validação do modelo Linear (curvas de loss, Roc e métricas de avaliação em formato .csv e latex).
  * [MyModel](DeepLearningModels/MyModel): Resultados do treinamento e validação do modelo Convolucional (curvas de loss, Roc e métricas de avaliação em formato .csv e latex).
  * [Label.csv](DeepLearningModels/Label.csv): Arquivo contendo as labels corretas referentes a cada amostra referentes ao arquivo [input.csv](DeepLearningModels/input.csv).
  * [MyModelLinear.py](DeepLearningModels/MyModelLinear.py): Implementação da rotina de treinamento e validação do Modelo Linear utilizando k-fold cross validation.
  * [dataset2.csv](DeepLearningModels/dataset2.csv): Dataset utilizado para treinamentos dos modelos antes do pré-processamento, que da origem aos arquivos [input.csv](DeepLearningModels/input.csv) e [Label.csv](DeepLearningModels/Label.csv).
  * [environment.yml](DeepLearningModels/environment.yml): Arquivo de configuração do ambiente de desenvolvimento utilizando miniconda.
  * [input.csv](DeepLearningModels/input.csv): Vetores de características normalizados utilizados como entrada para treinamento e validação dos modelos, os labels corretos referentes a cada classe estão presentes no arquivo [Label.csv](DeepLearningModels/Label.csv).
  * [modelUtils.py](DeepLearningModels/modelUtils.py): Implementação das arquiteturas dos modelos Linear e Convolucional.
  * [myModelTrain.py](DeepLearningModels/myModelTrain.py):Implementação da rotina de treinamento e validação do Modelo Convolucional utilizando k-fold cross validation.
  * [utils.py](DeepLearningModels/utils.py): Funções úteis para o pré-processamento do dataset entre outras atividades.
  


## Como Instalar as Dependências:
* Para os diretórios [data_extraction](data_extraction) e [ml_models](ml_models), as dependências podem ser instaladas com (de preferência em um ambiente virtual):
  * ```pip install -r requirements.txt```
* Para o diretório [DeepLearning Models](DeepLearningModels):
  * Instalar o ambiente de desenvolvimento miniconda através do link: https://docs.conda.io/en/latest/miniconda.html
  * No terminal do miniconda criar um ambiente a partir do arquivo environment.yml executando o comando ```conda env create -f environment.yml``` e aguardar a instalação das dependências.
  * Executar o comando ```conda activate myenv``` para entrar no ambiente recém criado.
  * Acessar os códigos através da IDE spyder, digitando o comando ```spyder``` no terminal do miniconda.

## Como Executar:
* Para executar os modelos baseados em Aprendizado de Máquina, basta digitar abaixo, após acessar o diretório [ml_models](ml_models):
  * **Florestas Aleatórias:** ```python model_random_forest.py```
  * **KNN:** ```python model_knn.py```
  * **Gradient Boosting:** ```python model_gradient_boosting.py```
* Ao fim da execução serão apresentadas as métricas dos modelos utilizando os hiperparâmetros já otimizados. 


