# ----------------------------------------------------------------------------------------------------------------------
# File containing the functions to pre-process the data and split them into training, validation and test sets.
# ----------------------------------------------------------------------------------------------------------------------

import os
import copy
import socket
import struct
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import train_test_split

from constants import SAVE_PATH
# ----------------------------------------------------------------------------------------------------------------------

# Function that divides data into training, validation and testing sets
def get_train_val_test_sets(dataset):

    # Separates features from labels
    labels = dataset["Label"].values
    features = dataset.drop(["Label"], axis=1).values

    # Gets 60% of the data for training
    X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.4, random_state=42)

    # Splits the remaining 40% into validation and test (50% for each)
    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)

    # Build the sets
    train_set = {"features": X_train, "labels": y_train}
    val_set = {"features": X_val, "labels": y_val}
    test_set = {"features": X_test, "labels": y_test}

    return(train_set, val_set, test_set)
# ----------------------------------------------------------------------------------------------------------------------

# Transform an IP into an integer
def ip2int(addr):
    return struct.unpack("!I", socket.inet_aton(addr))[0]
# ----------------------------------------------------------------------------------------------------------------------

# Transform an integer into an IP
def int2ip(addr):
    return socket.inet_ntoa(struct.pack("!I", addr))
# ----------------------------------------------------------------------------------------------------------------------

# Function that transforms the IPs into integers and Datetime into Timestamp
def transform_to_numeric_data(dataset):
    data = copy.deepcopy(dataset)
    data = data.drop(["Flow ID", "Subclass"], axis=1)

    # Transform the IPs into integers
    data["Src IP"] = [ip2int(ip) for ip in data["Src IP"]]
    data["Dst IP"] = [ip2int(ip) for ip in data["Dst IP"]]

    # Transform the datetime column into a Timestamp number
    data["Timestamp"] = pd.to_datetime(data["Timestamp"])
    data["Timestamp"] = data["Timestamp"].values.astype(np.int64) // 10 ** 9

    return(data)
# ----------------------------------------------------------------------------------------------------------------------

# Method: Feature Importance. It gives a score for each feature, the higher the score more important or relevant is the
# feature towards your output variable.
def feature_selection(data, labels, columns, num_features=10, verbose=False):

    # Use the model to get the best features
    model = ExtraTreesClassifier(random_state=42)
    model.fit(data, labels)
    feature_scores = model.feature_importances_.tolist()

    # Create a DataFrame for better visualization
    feat_importance = pd.DataFrame({"Features": columns, "Scores": feature_scores})

    # Get the 10 best features
    best_features = feat_importance.nlargest(num_features, "Scores")

    if(verbose):
        # Print 10 best features
        print("\nMethod: Feature Importance")
        print(best_features)

        # Plot it
        fig = go.Figure()
        fig.add_trace(go.Bar(y=best_features["Features"], x=best_features["Scores"], orientation="h"))

        # Define the title and axes labels
        fig.update_layout(title_text="Relação de Importância das Características Obtidas pelo Método de Seleção de Características",
                          xaxis_title_text="Porcentagens de Importância de Cada Característica",
                          yaxis_title_text="Características")

        fig.show()

    return(list(best_features.index))
# ----------------------------------------------------------------------------------------------------------------------

# Function that gets the selected features by column indexes
def get_selected_features_only(best_features, train_set, val_set, test_set):
    new_train_features = train_set["features"][:, best_features]
    new_val_features = val_set["features"][:, best_features]
    new_test_features = test_set["features"][:, best_features]

    new_train_set = {"features": new_train_features, "labels": train_set["labels"]}
    new_val_set = {"features": new_val_features, "labels": val_set["labels"]}
    new_test_set = {"features": new_test_features, "labels": test_set["labels"]}

    return(new_train_set, new_val_set, new_test_set)
# ----------------------------------------------------------------------------------------------------------------------

# Class that performs Min Max scaling
class MinMax:

    # Init the scaler (training set)
    def __init__(self, data):
        self.scaler = MinMaxScaler()
        self.scaler.fit(data)
    # ------------------------------------------------------------------------------------------------------------------

    # Scales data
    def apply_min_max(self, data):
        scaled_data = self.scaler.transform(data)
        return(scaled_data)
    # ------------------------------------------------------------------------------------------------------------------

    # Reverse scaling
    def reverse_min_max(self, data):
        reversed_data = self.scaler.inverse_transform(data)
        return(reversed_data)
    # ------------------------------------------------------------------------------------------------------------------

# ----------------------------------------------------------------------------------------------------------------------

# Function that combine all the preprocessing methods used
def preprocess_data(dataset, num_features=50, verbose=False):

    # Transform all type of features to numeric
    data = transform_to_numeric_data(dataset)

    # Get the column names
    columns = list(data.columns)
    columns.remove("Label")

    # Split the data into train, validation and test sets
    train_set, val_set, test_set = get_train_val_test_sets(data)

    # Apply the Feature Selection (validation set)
    best_features = feature_selection(val_set["features"], val_set["labels"], columns, num_features, verbose)

    # Select the chosen columns in Feature Selection
    fs_train_set, fs_val_set, fs_test_set = get_selected_features_only(best_features, train_set, val_set, test_set)

    # Apply MinMax Scale on training, validation and test sets
    for index in range(fs_train_set["features"].shape[1]):
        max_value = np.max(fs_train_set["features"][:, index])

        # Don't apply the scale on boolean data
        if(max_value > 1):

            # Format the data of the selected column
            train_column = np.array([[elem] for elem in fs_train_set["features"][:, index]])
            val_column = np.array([[elem] for elem in fs_val_set["features"][:, index]])
            test_column = np.array([[elem] for elem in fs_test_set["features"][:, index]])

            # Scale training set
            min_max_scaler = MinMax(train_column)
            scaled_data = min_max_scaler.apply_min_max(train_column)
            scaler_to_array = [elem[0] for elem in scaled_data]
            fs_train_set["features"][:, index] = scaler_to_array

            # Scale validation set
            min_max_scaler = MinMax(val_column)
            scaled_data = min_max_scaler.apply_min_max(val_column)
            scaler_to_array = [elem[0] for elem in scaled_data]
            fs_val_set["features"][:, index] = scaler_to_array

            # Scale test set
            min_max_scaler = MinMax(test_column)
            scaled_data = min_max_scaler.apply_min_max(test_column)
            scaler_to_array = [elem[0] for elem in scaled_data]
            fs_test_set["features"][:, index] = scaler_to_array

    return(fs_train_set, fs_val_set, fs_test_set)
# ----------------------------------------------------------------------------------------------------------------------

if __name__ == "__main__":

    # Read the dataset
    path = os.path.join(SAVE_PATH, "dataset.csv")
    dataset = pd.read_csv(path)

    # Preprocess the data
    train_set, val_set, test_set = preprocess_data(dataset, verbose=True)
# ----------------------------------------------------------------------------------------------------------------------
